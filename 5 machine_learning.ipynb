{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from typing import Literal, Union\n",
    "\n",
    "from imblearn.over_sampling import ADASYN, SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay\n",
    "import shap\n",
    "\n",
    "# Set font family to a font that supports CJK characters\n",
    "from matplotlib import rcParams\n",
    "rcParams['font.sans-serif'] = ['PingFang HK']\n",
    "rcParams['axes.unicode_minus'] = False  # Ensure minus sign renders correctly\n",
    "\n",
    "# Ignore warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "LEARNING_RATE = 0.005\n",
    "RANDOM_STATE = 101\n",
    "\n",
    "# Study targets\n",
    "TARGET_COLS = [\"Premature Rupture of Membranes\",    # 胎膜早破\n",
    "               \"Fetal Distress\",                    # 胎儿宫内窘迫\n",
    "               \"Macrosomia\",                        # 巨大儿\n",
    "               \"Amniotic Fluid Contamination\"]      # 羊水污染"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path to save all model metrics\n",
    "BASE_DIR = os.path.join(os.getcwd(), \"Model_Metrics\")\n",
    "if not os.path.isdir(BASE_DIR):\n",
    "    os.makedirs(BASE_DIR)\n",
    "\n",
    "#Path to load imputed datasets    \n",
    "IMPUTED_DATASETS_DIR = os.path.join(os.getcwd(), \"MICE\", \"Imputed_Datasets\")\n",
    "if not os.path.isdir(IMPUTED_DATASETS_DIR):\n",
    "    raise IOError(\"Imputed datasets directory not found. Run MICE script first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize and Wrap Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to initialize and wrap models\n",
    "def get_models(is_balanced: bool=True, L1_regularization: float=1.0, L2_regularization: float=1.0) -> dict:\n",
    "    ''' \n",
    "    Returns a dictionary `{Model_Name: Model}` with new instances of:\n",
    "        * \"XGB\" - Extreme Gradient Boosting Classifier (XGBoost).\n",
    "        * \"LGBM\" - Light Gradient Boosting Machine Classifier (LightGBM).\n",
    "        * \"HistGB\" - Histogram-Based Gradient Boosting Classifier (HistGB).\n",
    "        \n",
    "    If working with imbalanced datasets, set `is_balanced=False`.\n",
    "    '''\n",
    "    \n",
    "    # Initialize XGBoost Classifier\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=5,                  # tunable\n",
    "        learning_rate=LEARNING_RATE,  # tunable\n",
    "        subsample=0.8,                # tunable\n",
    "        colsample_bytree=0.8,         # tunable\n",
    "        random_state=RANDOM_STATE,     # added for reproducibility\n",
    "        \n",
    "        scale_pos_weight=1 if is_balanced else 8,           # tunable\n",
    "        reg_alpha=L1_regularization,                 # tunable L1 regularization\n",
    "        reg_lambda=L2_regularization,                # tunable L2 regularization\n",
    "        eval_metric='aucpr',    # tunable, default is 'logloss'\n",
    "    )\n",
    "    \n",
    "    # Initialize LightGBM Classifier\n",
    "    lgbm_model = lgb.LGBMClassifier(\n",
    "        n_estimators=200,\n",
    "        learning_rate=LEARNING_RATE,  # tunable\n",
    "        max_depth=5,                 # tunable\n",
    "        subsample=0.8,                # tunable\n",
    "        colsample_bytree=0.8,         # tunable\n",
    "        random_state=RANDOM_STATE,     # added for reproducibility\n",
    "        # force_col_wise=True,        #Use if there are more columns than rows\n",
    "        verbose=-1,              #Requires TESTING\n",
    "        \n",
    "        class_weight=None if is_balanced else 'balanced',  # tunable\n",
    "        # is_unbalance= not is_balanced,                     # tunable alternative to class_weight, choose one\n",
    "        boosting_type='goss' if is_balanced else 'dart',   # tunable, DART requires more iterations and is slower\n",
    "        reg_alpha=L1_regularization,                 # tunable L1 regularization\n",
    "        reg_lambda=L2_regularization,                # tunable L2 regularization\n",
    "    )\n",
    "    \n",
    "    # Initialize HistGradientBoostingClassifier\n",
    "    hist_model = HistGradientBoostingClassifier(\n",
    "        max_iter=200,                 # tunable\n",
    "        learning_rate=LEARNING_RATE,  # tunable\n",
    "        max_depth=5,                  # tunable\n",
    "        min_samples_leaf=30,          # tunable\n",
    "        random_state=RANDOM_STATE,    # added for reproducibility\n",
    "        \n",
    "        class_weight=None if is_balanced else 'balanced',  # tunable\n",
    "        l2_regularization=L2_regularization,          # tunable\n",
    "        scoring='loss' if is_balanced else 'balanced_accuracy',       # tunable\n",
    "    )\n",
    "    \n",
    "    # Wrap classifiers with MultiOutputClassifier\n",
    "    models = {\n",
    "        \"XGB\": xgb_model,\n",
    "        \"LGBM\": lgbm_model,\n",
    "        \"HistGB\": hist_model\n",
    "    }\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Functions to split, scale, and resample data in a single dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to split data into train and test\n",
    "def _split_data(features, target, test_size):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=test_size, random_state=RANDOM_STATE, stratify=target)   \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# function to standardize the data\n",
    "def _standardize_data(train_features, test_features):\n",
    "    scaler = StandardScaler()\n",
    "    train_scaled = scaler.fit_transform(train_features)\n",
    "    test_scaled = scaler.transform(test_features)\n",
    "    return train_scaled, test_scaled\n",
    "\n",
    "# Over-sample minority class (Positive cases) and return several single target datasets\n",
    "def _resample(X_train_scaled: np.ndarray, y_train: pd.Series, \n",
    "                strategy: Literal[r\"ADASYN\", r'SMOTE', r'RANDOM', r'UNDERSAMPLE', None]=None):\n",
    "    ''' \n",
    "    Oversample minority class or undersample majority class.\n",
    "    \n",
    "    Returns a Tuple `(Features: nD-Array, Target: 1D-array)`\n",
    "    '''\n",
    "    if strategy is None:\n",
    "        return X_train_scaled, y_train\n",
    "    elif strategy == 'SMOTE':\n",
    "        resample_algorithm = SMOTE(random_state=RANDOM_STATE, k_neighbors=3)\n",
    "    elif strategy == 'RANDOM':\n",
    "        resample_algorithm = RandomOverSampler(random_state=RANDOM_STATE)\n",
    "    elif strategy == 'UNDERSAMPLE':\n",
    "        resample_algorithm = RandomUnderSampler(random_state=RANDOM_STATE)\n",
    "    elif strategy == 'ADASYN':\n",
    "        resample_algorithm = ADASYN(random_state=RANDOM_STATE, n_neighbors=3)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid resampling strategy: {strategy}\")\n",
    "    \n",
    "    X_res, y_res = resample_algorithm.fit_resample(X_train_scaled, y_train)\n",
    "    return X_res, y_res\n",
    "\n",
    "\n",
    "# DATASET PIPELINE\n",
    "def dataset_pipeline(df_features: pd.DataFrame, df_target: pd.Series, resample_strategy: Union[str, None], test_size: float=0.2, debug: bool=False):\n",
    "    ''' \n",
    "    1. Make Train/Test splits\n",
    "    2. Standardize Train and Test Features\n",
    "    3. Oversample imbalanced classes and create individual DataFrames\n",
    "    \n",
    "    Return a processed Tuple: (X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    `(nD-array, 1D-array, nD-array, Series)`\n",
    "    '''\n",
    "    #DEBUG\n",
    "    if debug:\n",
    "        print(f\"Split Dataframes Shapes - Features DF: {df_features.shape}, Target DF: {df_target.shape}\")\n",
    "        unique_values = df_target.unique()  # Get unique values for the target column\n",
    "        print(f\"\\tUnique values for '{df_target.name}': {unique_values}\")\n",
    "    \n",
    "    #Train test split\n",
    "    X_train, X_test, y_train, y_test = _split_data(features=df_features, target=df_target, test_size=test_size)\n",
    "    \n",
    "    #DEBUG\n",
    "    if debug:\n",
    "        print(f\"Shapes after train test split - X_train: {X_train.shape}, y_train: {y_train.shape}, X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "    \n",
    "    # Standardize    \n",
    "    X_train_scaled, X_test_scaled = _standardize_data(train_features=X_train, test_features=X_test)\n",
    "    \n",
    "    #DEBUG\n",
    "    if debug:\n",
    "        print(f\"Shapes after scaling features - X_train: {X_train_scaled.shape}, y_train: {y_train.shape}, X_test: {X_test_scaled.shape}, y_test: {y_test.shape}\")\n",
    " \n",
    "    # Scale\n",
    "    X_train_oversampled, y_train_oversampled = _resample(X_train_scaled=X_train_scaled, y_train=y_train, strategy=resample_strategy)\n",
    "    \n",
    "    #DEBUG\n",
    "    if debug:\n",
    "        print(f\"Shapes after resampling - X_train: {X_train_oversampled.shape}, y_train: {y_train_oversampled.shape}, X_test: {X_test_scaled.shape}, y_test: {y_test.shape}\")\n",
    "    \n",
    "    return X_train_oversampled, y_train_oversampled, X_test_scaled, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Functions to train and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer function\n",
    "def _train_model(model, train_features, train_target):\n",
    "    model.fit(train_features, train_target)\n",
    "    return model\n",
    "\n",
    "# function to evaluate the model and save metrics\n",
    "def _evaluate_model(model, model_name: str, dataset_id: str, x_test_scaled: np.ndarray, single_y_test: np.ndarray, target_id: str):\n",
    "    model_dir = os.path.join(BASE_DIR, model_name)\n",
    "    if not os.path.isdir(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "        \n",
    "    dataset_dir = os.path.join(model_dir, dataset_id)\n",
    "    if not os.path.isdir(dataset_dir):\n",
    "        os.makedirs(dataset_dir)\n",
    "    \n",
    "    y_pred = model.predict(x_test_scaled)\n",
    "    accuracy = accuracy_score(single_y_test, y_pred)\n",
    "    report = classification_report(single_y_test, y_pred, target_names=[\"Negative\", \"Positive\"])\n",
    "    \n",
    "    report_path = os.path.join(dataset_dir, \"classification_reports.txt\")\n",
    "    # Open the file in append mode\n",
    "    with open(report_path, \"a\") as f:\n",
    "        f.write(f\"{model_name} - {target_id}            Accuracy: {accuracy:.2f}\\n\")\n",
    "        f.write(f\"Classification Report:\\n\")\n",
    "        f.write(report)\n",
    "        f.write(\"\\n\\n\")  # Extra spacing between entries\n",
    "        \n",
    "    #Generate confusion matrix\n",
    "    disp = ConfusionMatrixDisplay.from_predictions(\n",
    "        y_true=single_y_test, y_pred=y_pred,\n",
    "        display_labels=[\"Negative\", \"Positive\"],\n",
    "        cmap=plt.cm.Blues,\n",
    "        normalize=\"true\"\n",
    "    )\n",
    "    plt.title(f\"{model_name} - Confusion Matrix for {target_id}\")\n",
    "    plt.savefig(os.path.join(dataset_dir, f\"Confusion_Matrix_{target_id}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "# Get SHAP values\n",
    "def get_shap_values(model, model_name: str, dataset_id: str, features_to_explain: np.ndarray, feature_names: list[str], target_id: str):\n",
    "    \"\"\"\n",
    "    Generate and save SHAP summary plots for each target in a multi-output model.\n",
    "    \n",
    "    `features`: Use scaled values if the model was trained on scaled values.\n",
    "    \n",
    "    * Use `X_train` (or a subsample of it) to see how the model explains the data it was trained on. (Default)\n",
    "\t* Use `X_test` (or a hold-out set) to see how the model explains unseen data.\n",
    "\t* Use the entire dataset to get the global view. \n",
    "    \"\"\"    \n",
    "    # Check directories\n",
    "    model_dir = os.path.join(BASE_DIR, model_name)\n",
    "    if not os.path.isdir(model_dir):\n",
    "        raise IOError(f\"{model_dir} does not exist.\\nTrain and evaluate the model first.\")\n",
    "    \n",
    "    dataset_dir = os.path.join(model_dir, dataset_id)\n",
    "    if not os.path.isdir(dataset_dir):\n",
    "        raise IOError(f\"{dataset_dir} does not exist.\\nTrain and evaluate the model first.\")\n",
    "    \n",
    "\n",
    "    # SHAP explainer for the target estimator\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    \n",
    "    # Compute SHAP values\n",
    "    shap_values = explainer.shap_values(features_to_explain)\n",
    "    \n",
    "    # TEST, WARNING for LGBM, list of arrays [negative, positive]\n",
    "    # if isinstance(model, lgb.LGBMClassifier):\n",
    "    #     shap_values = shap_values[1]\n",
    "        \n",
    "\n",
    "    shap.summary_plot(\n",
    "        shap_values, \n",
    "        features_to_explain, \n",
    "        feature_names=feature_names,\n",
    "        plot_type=\"bar\",\n",
    "        show=False\n",
    "    )\n",
    "    \n",
    "    plt.gcf().set_size_inches(12, 20)  #(width, height)\n",
    "    plt.xlabel(\"Mean Absolute SHAP Value\")\n",
    "    plt.title(f\"{model_name} - SHAP Summary for {target_id}\")\n",
    "    file_name = os.path.join(dataset_dir, f\"SHAP_{target_id}.png\")\n",
    "    plt.savefig(file_name, bbox_inches='tight')\n",
    "    plt.close()\n",
    "        \n",
    "        \n",
    "# TRAIN TEST PIPELINE\n",
    "def train_test_pipeline(model, model_name: str, dataset_id: str, \n",
    "             train_features: np.ndarray, train_target: np.ndarray,\n",
    "             test_features: np.ndarray, test_target: np.ndarray,\n",
    "             feature_names: list[str], target_id: str,\n",
    "             debug: bool=False):\n",
    "    ''' \n",
    "    1. Train model.\n",
    "    2. Evaluate model.\n",
    "    3. SHAP values.\n",
    "    \n",
    "    Returns: Tuple(Trained model, Test-set Predictions)\n",
    "    '''\n",
    "    print(f\"\\tWorking with Model: {model_name} for Target: {target_id}...\")\n",
    "    trained_model = _train_model(model=model, train_features=train_features, train_target=train_target)\n",
    "    y_pred = _evaluate_model(model=trained_model, model_name=model_name, dataset_id=dataset_id, \n",
    "                             x_test_scaled=test_features, single_y_test=test_target, target_id=target_id)\n",
    "    get_shap_values(model=trained_model, model_name=model_name, dataset_id=dataset_id,\n",
    "                    features_to_explain=train_features, feature_names=feature_names, target_id=target_id)\n",
    "    print(f\"\\tprocess complete.\")\n",
    "    return trained_model, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Function to Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load imputed datasets as a generator\n",
    "def yield_imputed_dataframe(datasets_dir: str):\n",
    "    '''\n",
    "    Yields a tuple `(dataframe, dataframe_name)`\n",
    "    '''\n",
    "    dataset_filenames = [dataset for dataset in os.listdir(datasets_dir) if dataset.endswith(\".csv\")]\n",
    "    if not dataset_filenames:\n",
    "        raise IOError(f\"No imputed datasets have been found at {datasets_dir}\")\n",
    "    \n",
    "    for dataset_filename in dataset_filenames:\n",
    "        full_path = os.path.join(datasets_dir, dataset_filename)\n",
    "        df = pd.read_csv(full_path)\n",
    "        #remove extension\n",
    "        filename = os.path.splitext(os.path.basename(dataset_filename))[0]\n",
    "        print(f\"Working on file: {filename}\")\n",
    "        yield (df, filename)\n",
    "        \n",
    "#Split a dataset into features and targets datasets\n",
    "def dataset_yielder(df: pd.DataFrame, target_cols: list[str]=TARGET_COLS):\n",
    "    ''' \n",
    "    Yields one Tuple at a time: `(df_features, df_target, feature_names, target_name)`\n",
    "    '''\n",
    "    df_features = df.drop(columns=target_cols)\n",
    "    feature_names = df_features.columns\n",
    "    \n",
    "    for target_col in target_cols:\n",
    "        df_target = df[target_col]\n",
    "        yield (df_features, df_target, feature_names, target_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main function\n",
    "def main(imputed_datasets_dir: str, target_cols: list[str], resample_strategy: Literal[r\"ADASYN\", r'SMOTE', r'RANDOM', r'UNDERSAMPLE', None]=None, \n",
    "         test_size: float=0.2, debug:bool=False, L1_regularization: float=1.0, L2_regularization: float=1.0):\n",
    "    #Increase L1 and L2 if model is overfitting\n",
    "    #Yield imputed dataset\n",
    "    for dataframe, dataframe_name in yield_imputed_dataframe(imputed_datasets_dir):\n",
    "        #Yield features dataframe and target dataframe\n",
    "        for df_features, df_target, feature_names, target_name in dataset_yielder(df=dataframe, target_cols=target_cols):\n",
    "            #Dataset pipeline\n",
    "            X_train, y_train, X_test, y_test = dataset_pipeline(df_features=df_features, df_target=df_target, resample_strategy=resample_strategy, test_size=test_size, debug=debug)\n",
    "            #Get models\n",
    "            models_dict = get_models(is_balanced=False if resample_strategy is None else True, L1_regularization=L1_regularization, L2_regularization=L2_regularization)\n",
    "            for model_name, model in models_dict.items():\n",
    "                train_test_pipeline(model=model, model_name=model_name, dataset_id=dataframe_name,\n",
    "                                    train_features=X_train, train_target=y_train,\n",
    "                                    test_features=X_test, test_target=y_test,\n",
    "                                    feature_names=feature_names,target_id=target_name,\n",
    "                                    debug=debug)\n",
    "    print(\"\\nTraining and evaluation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on file: imputed_01\n",
      "\tWorking with Model: XGB for Target: Premature Rupture of Membranes...\n",
      "\tprocess complete.\n",
      "\tWorking with Model: LGBM for Target: Premature Rupture of Membranes...\n",
      "\tprocess complete.\n",
      "\tWorking with Model: HistGB for Target: Premature Rupture of Membranes...\n",
      "\tprocess complete.\n",
      "\tWorking with Model: XGB for Target: Fetal Distress...\n",
      "\tprocess complete.\n",
      "\tWorking with Model: LGBM for Target: Fetal Distress...\n",
      "\tprocess complete.\n",
      "\tWorking with Model: HistGB for Target: Fetal Distress...\n",
      "\tprocess complete.\n",
      "\tWorking with Model: XGB for Target: Macrosomia...\n",
      "\tprocess complete.\n",
      "\tWorking with Model: LGBM for Target: Macrosomia...\n",
      "\tprocess complete.\n",
      "\tWorking with Model: HistGB for Target: Macrosomia...\n",
      "\tprocess complete.\n",
      "\tWorking with Model: XGB for Target: Amniotic Fluid Contamination...\n",
      "\tprocess complete.\n",
      "\tWorking with Model: LGBM for Target: Amniotic Fluid Contamination...\n",
      "\tprocess complete.\n",
      "\tWorking with Model: HistGB for Target: Amniotic Fluid Contamination...\n",
      "\tprocess complete.\n",
      "Working on file: imputed_02\n",
      "\tWorking with Model: XGB for Target: Premature Rupture of Membranes...\n",
      "\tprocess complete.\n",
      "\tWorking with Model: LGBM for Target: Premature Rupture of Membranes...\n",
      "\tprocess complete.\n",
      "\tWorking with Model: HistGB for Target: Premature Rupture of Membranes...\n",
      "\tprocess complete.\n",
      "\tWorking with Model: XGB for Target: Fetal Distress...\n",
      "\tprocess complete.\n",
      "\tWorking with Model: LGBM for Target: Fetal Distress...\n",
      "\tprocess complete.\n",
      "\tWorking with Model: HistGB for Target: Fetal Distress...\n",
      "\tprocess complete.\n",
      "\tWorking with Model: XGB for Target: Macrosomia...\n",
      "\tprocess complete.\n",
      "\tWorking with Model: LGBM for Target: Macrosomia...\n",
      "\tprocess complete.\n",
      "\tWorking with Model: HistGB for Target: Macrosomia...\n",
      "\tprocess complete.\n",
      "\tWorking with Model: XGB for Target: Amniotic Fluid Contamination...\n",
      "\tprocess complete.\n",
      "\tWorking with Model: LGBM for Target: Amniotic Fluid Contamination...\n",
      "\tprocess complete.\n",
      "\tWorking with Model: HistGB for Target: Amniotic Fluid Contamination...\n",
      "\tprocess complete.\n",
      "Working on file: imputed_03\n",
      "\tWorking with Model: XGB for Target: Premature Rupture of Membranes...\n",
      "\tprocess complete.\n",
      "\tWorking with Model: LGBM for Target: Premature Rupture of Membranes...\n",
      "\tprocess complete.\n",
      "\tWorking with Model: HistGB for Target: Premature Rupture of Membranes...\n",
      "\tprocess complete.\n",
      "\tWorking with Model: XGB for Target: Fetal Distress...\n",
      "\tprocess complete.\n",
      "\tWorking with Model: LGBM for Target: Fetal Distress...\n",
      "\tprocess complete.\n",
      "\tWorking with Model: HistGB for Target: Fetal Distress...\n",
      "\tprocess complete.\n",
      "\tWorking with Model: XGB for Target: Macrosomia...\n",
      "\tprocess complete.\n",
      "\tWorking with Model: LGBM for Target: Macrosomia...\n",
      "\tprocess complete.\n",
      "\tWorking with Model: HistGB for Target: Macrosomia...\n",
      "\tprocess complete.\n",
      "\tWorking with Model: XGB for Target: Amniotic Fluid Contamination...\n",
      "\tprocess complete.\n",
      "\tWorking with Model: LGBM for Target: Amniotic Fluid Contamination...\n",
      "\tprocess complete.\n",
      "\tWorking with Model: HistGB for Target: Amniotic Fluid Contamination...\n",
      "\tprocess complete.\n",
      "\n",
      "Training and evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "main(imputed_datasets_dir=IMPUTED_DATASETS_DIR, target_cols=TARGET_COLS, test_size=0.2, debug=False,\n",
    "     resample_strategy=None, L1_regularization=1.5, L2_regularization=1.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thyroid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
